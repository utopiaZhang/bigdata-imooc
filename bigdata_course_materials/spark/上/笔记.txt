一主两从集群
主节点：bigdata01
从节点：bigdata02、bigdata03

注意：需要确保这几台机器的基础环境是OK的



//第二步：加载数据
val linesRDD = sc.textFile("D:\\hello.txt")
linesRDD中的数据是这样的：
hello you
hello me


//第三步：对数据进行切割，把一行数据切分成一个一个的单词
val wordsRDD = linesRDD.flatMap(_.split(" "))
wordsRDD中的数据是这样的：
hello
you
hello
me


//第四步：迭代words，将每个word转换为(word,1)这种形式
val pairRDD = wordsRDD.map((_,1))
pairRDD中的数据是这样的:
(hello,1)
(you,1)
(hello,1)
(me,1)

//第五步：根据key(其实就是word)进行分组聚合统计
val wordCountRDD = pairRDD.reduceByKey(_ + _)
wordCountRDD中的数据是这样的:
(hello,2)
(you,1)
(me,1)






val dataRDD = sc.parallelize(Array(1,2,3,4,5))
val mapRDD = dataRDD.map(...)
mapRDD.foreach(...)
mapRDD.saveAsTextFile(...)
mapRDD.collect(...)


没有开启cache()
第一次耗时：11718
第二次耗时：5396

开启cache()
第一次耗时：17623
第二次耗时：442




TopN主播统计
1：首先获取两份数据中的核心字段，使用fastjson包解析数据
主播开播记录(video_info.log):主播ID：uid，直播间ID：vid，大区：area
(vid,(uid,area))
用户送礼记录(gift_record.log)：直播间ID：vid，金币数量：gold
(vid,gold)

这样的话可以把这两份数据关联到一块就能获取到大区、主播id、金币这些信息了，使用直播间vid进行关联

2：对用户送礼记录数据进行聚合，对相同vid的数据求和
因为用户可能在一次直播中给主播送多次礼物
(vid,gold_sum)

3：把这两份数据join到一块，vid作为join的key
(vid,((uid,area),gold_sum))

4：使用map迭代join之后的数据，最后获取到uid、area、gold_sum字段
由于一个主播一天可能会开播多次，后面需要基于uid和area再做一次聚合，所以把数据转换成这种格式

uid和area是一一对应的，一个人只能属于大区
((uid,area),gold_sum)

5：使用reduceByKey算子对数据进行聚合
((uid,area),gold_sum_all)

6：接下来对需要使用groupByKey对数据进行分组，所以先使用map进行转换
因为我们要分区统计TopN，所以要根据大区分组
map：(area,(uid,gold_sum_all))
groupByKey: area,<(uid,gold_sum_all),(uid,gold_sum_all),(uid,gold_sum_all)>

7：使用map迭代每个分组内的数据，按照金币数量倒序排序，取前N个，最终输出area,topN
这个topN其实就是把前几名主播的id还有金币数量拼接成一个字符串
(area,topN)

8：使用foreach将结果打印到控制台，多个字段使用制表符分割
area	topN






作业：
把spark任务的输入数据上传到hdfs上面，然后把结果保存到hdfs上的/data/topn目录下
针对spark任务打jar包，使用on yarn模式提交到集群执行

注意：针对fastjson这个依赖，有两种选择
1：把这个依赖一起打进jar包里面
2：在提交任务的时候动态指定这个依赖jar包






